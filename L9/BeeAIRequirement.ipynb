{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e138af4c",
   "metadata": {},
   "source": [
    "# Lesson 9 - Creating an Agentic multi-agent system using A2A with BeeAI Framework\n",
    "\n",
    "In this final code lesson, you will create a comprehensive \"Healthcare Concierge\" system. You will use the **BeeAI Framework** to orchestrate all three agents you have built so far (Policy, Research, and Provider). The BeeAI `RequirementAgent` will act as a router, deciding which A2A agent to hand off to based on the user's complex query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbb197-2b05-45e3-a5eb-0e9257bcf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from helpers import authenticate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14212237",
   "metadata": {},
   "source": [
    "## 9.1. Start All Agent Servers\n",
    "\n",
    "Ensure all three terminals are running their respective agents:\n",
    "1.  **Policy Agent** (Lesson 3)\n",
    "2.  **Research Agent** (Lesson 5)\n",
    "3.  **Provider Agent** (Lesson 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb88c95-3057-4477-beec-bad987ea7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "url = os.environ.get(\"DLAI_LOCAL_URL\").format(port=8888)\n",
    "IFrame(f\"{url}terminals/1\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48527c-8584-4011-a954-acd489035842",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(f\"{url}terminals/2\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8d64b-dfcd-481a-9508-5dc3e8ffb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(f\"{url}terminals/3\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19787f",
   "metadata": {},
   "source": [
    "## 9.2. Define BeeAI Components\n",
    "\n",
    "Here you will:\n",
    "1.  Import BeeAI framework components, including `RequirementAgent` and `HandoffTool`.\n",
    "2.  Define `A2AAgent` instances for each of your running servers.\n",
    "3.  Use `check_agent_exists()` to fetch the metadata (AgentCard) from each server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff7b46-1481-48cc-a7c7-6544ba3f01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.adapters.a2a import A2AServer, A2AServerConfig\n",
    "from beeai_framework.adapters.a2a.agents import A2AAgent\n",
    "from beeai_framework.adapters.vertexai import VertexAIChatModel\n",
    "from beeai_framework.agents.requirement import RequirementAgent\n",
    "from beeai_framework.agents.requirement.requirements.conditional import (\n",
    "    ConditionalRequirement,\n",
    ")\n",
    "from beeai_framework.memory import UnconstrainedMemory\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "from beeai_framework.serve.utils import LRUMemoryManager\n",
    "from beeai_framework.tools.handoff import HandoffTool\n",
    "from beeai_framework.tools.think import ThinkTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26a043-db74-47ec-9cb9-3da46e4e77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bf7f8-b214-4da7-9828-87b465d50530",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "_, project_id = authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f15c5-9150-466b-9b0a-af5fb7a2040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.environ.get(\"AGENT_HOST\")\n",
    "policy_port = os.environ.get(\"POLICY_AGENT_PORT\")\n",
    "research_port = os.environ.get(\"RESEARCH_AGENT_PORT\")\n",
    "provider_port = os.environ.get(\"PROVIDER_AGENT_PORT\")\n",
    "healthcare_agent_port = int(os.environ.get(\"HEALTHCARE_AGENT_PORT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a115368-8853-4248-a6d7-dc98e76051ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{policy_port}\", memory=UnconstrainedMemory()\n",
    ")\n",
    "# Run `check_agent_exists()` to fetch and populate AgentCard\n",
    "asyncio.run(policy_agent.check_agent_exists())\n",
    "print(\"\\tℹ️\", f\"{policy_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42619d7-24e0-402d-9535-243eeae5e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{research_port}\", memory=UnconstrainedMemory()\n",
    ")\n",
    "asyncio.run(research_agent.check_agent_exists())\n",
    "print(\"\\tℹ️\", f\"{research_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0895c4-c3fd-4782-a330-1124daf42d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{provider_port}\", memory=UnconstrainedMemory()\n",
    ")\n",
    "asyncio.run(provider_agent.check_agent_exists())\n",
    "print(\"\\tℹ️\", f\"{provider_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f691f",
   "metadata": {},
   "source": [
    "## 9.3. Configure the Orchestrator (Healthcare Concierge)\n",
    "\n",
    "You will now configure the `RequirementAgent`. This agent uses a `VertexAIChatModel` and is equipped with `HandoffTool`s connected to your A2A agents. The instructions explicitly guide the LLM on how to use each specific agent (Research for conditions, Policy for insurance, Provider for doctors) to answer multi-part questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac7ebf-ad87-4dd3-b5ac-7a869d23cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthcare_agent = RequirementAgent(\n",
    "    name=\"Healthcare Agent\",\n",
    "    description=\"A personal concierge for Healthcare Information, customized to your policy.\",\n",
    "    llm=VertexAIChatModel(\n",
    "        model_id=\"gemini-2.5-flash\",\n",
    "        project=project_id,\n",
    "        location=\"global\",\n",
    "        allow_parallel_tool_calls=True,\n",
    "    ),\n",
    "    tools=[\n",
    "        ThinkTool(),\n",
    "        HandoffTool(\n",
    "            policy_agent,\n",
    "            name=policy_agent.name,\n",
    "            description=policy_agent.agent_card.description,\n",
    "        ),\n",
    "        HandoffTool(\n",
    "            research_agent,\n",
    "            name=research_agent.name,\n",
    "            description=research_agent.agent_card.description,\n",
    "        ),\n",
    "        HandoffTool(\n",
    "            provider_agent,\n",
    "            name=provider_agent.name,\n",
    "            description=provider_agent.agent_card.description,\n",
    "        ),\n",
    "    ],\n",
    "    requirements=[\n",
    "        ConditionalRequirement(ThinkTool, consecutive_allowed=False),\n",
    "    ],\n",
    "    role=\"Healthcare Concierge\",\n",
    "    instructions=(\n",
    "        f\"\"\"You are a concierge for healthcare services. Your task is to handoff to one or more agents to answer questions and provide a detailed summary of their answers. Be sure that all of their questions are answered before responding.\n",
    "        Use `{research_agent.name}` for research about their condition. Provide this research to the user.\n",
    "        Use `{policy_agent.name}` for information about their specific Health Insurance Policy.\n",
    "        Use `{provider_agent.name}` for finding providers in their location.\n",
    "\n",
    "        IMPORTANT: When returning answers about providers, only output providers from `{provider_agent.name}` and only provide insurance information based on the results from `{policy_agent.name}`.\n",
    "\n",
    "        In your output, put which agent gave you the information.\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\tℹ️\", f\"{healthcare_agent.meta.name} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223bf0c",
   "metadata": {},
   "source": [
    "## 9.4. Run the Full Workflow\n",
    "\n",
    "Test the system with a complex query that requires information from all three sub-agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afacd17-fa19-4745-ab26-6f8e3c746e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await healthcare_agent.run(\n",
    "    \"I'm based in Austin, TX. How do I get mental health therapy near me and what does my insurance cover?\"\n",
    ")\n",
    "display(Markdown(response.last_message.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0d12c",
   "metadata": {},
   "source": [
    "## 9.5. Serve the Concierge Agent\n",
    "\n",
    "Finally, you can register this high-level \"Concierge\" agent itself as an A2A server. This demonstrates the recursive power of A2A: an agent composed of other A2A agents can itself be exposed as an A2A agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the agent with the A2A server and run the HTTP server\n",
    "# we use LRU memory manager to keep limited amount of sessions in the memory\n",
    "server = A2AServer(\n",
    "    config=A2AServerConfig(host=host, port=healthcare_agent_port),\n",
    "    memory_manager=LRUMemoryManager(maxsize=100),\n",
    ").register(healthcare_agent)\n",
    "\n",
    "server.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b485fe",
   "metadata": {},
   "source": [
    "## 9.6. Resources\n",
    "\n",
    "- [BeeAI Framework](https://framework.beeai.dev/introduction/welcome)\n",
    "- [Requirement Agent](https://framework.beeai.dev/modules/agents/requirement-agent)\n",
    "- [BeeAI Framework GitHub](https://github.com/i-am-bee/beeai-framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd4d7c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ⬇ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
